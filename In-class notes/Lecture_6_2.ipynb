{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.2 Programming a Deep Neural Network from Scratch with Python\n",
    "\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_6/Lecture_6_2.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## MNIST Data Set\n",
    "The [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database) consists of 70,000 images of hand written digits, 60,000 of which are typically used as labeled training examples and the other 10,000 used for testing your learning model on. The following picture represent a sample of some of the images.\n",
    "\n",
    "<img src=\"MnistExamples.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can load this dataset with the ```tensorflow.keras``` package. Load this data by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the keras module to import the necessary data \n",
    "(train_X, train_y), (test_X, test_y) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The data structures ```train_x``` and ```test_x``` are stored as 3 dimensional tensors. \n",
    " \n",
    "<img src=\"order-3-tensor.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "This can be varified by finding the shape of these variables. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(train_X) =(60000, 28, 28)\n",
      "np.shape(test_X) = (10000, 28, 28) \n",
      "\n",
      "np.shape(train_X[0]) = (28, 28)\n",
      "np.shape(test_X[0]) = (28, 28) \n",
      "\n",
      "train_X[0] = [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# This cell should be run inplace of the cell directly above\n",
    "print(f\"np.shape(train_X) ={np.shape(train_X)}\")\n",
    "print(f\"np.shape(test_X) = {np.shape(test_X)} \\n\")\n",
    "\n",
    "print(f\"np.shape(train_X[0]) = {np.shape(train_X[0])}\")\n",
    "print(f\"np.shape(test_X[0]) = {np.shape(test_X[0])} \\n\")\n",
    "\n",
    "print(f\"train_X[0] = {train_X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Each image is comprised of a $28\\times 28$ grey scaled grid of pixel values. These values are floating point numbers in the interval $(0,1)$, where darker pixels will have values closer to $1$ and lighter pixels will have values closer to $0$. The following image represents one such example. \n",
    "\n",
    "<img src=\"MNIST-Matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "We can view the image of one of these matrices by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_y[0] = } \\n\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Image Flattening\n",
    "\n",
    "Simple **dense neural networks** pass feature vectors into the 0-th (the first) layer of the computational graph represented by the neural network structure as column vectors. This 0-th layer is essentially the same as with single neuron models. In order to feed our images into such a network we must **flatten** the matrix into a column vector.\n",
    "\n",
    "<img src=\"flatten.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can do this for each image matrix that we are considering by calling the ```flatten()``` method together with the ```reshape(784, 1)``` method to insure it is a column vector. Note, that each image matrix is 28 by 28, and so, $784 = 28 \\times 28$ is the number of rows in the flattened matrix column vector. The numerical values in the flattened training and testing data matrices vary between 0 and 255. These large differences in possible values can lead to problems when training the weights and the biases of the neural network. A quick and dirty fix will be to scale all data to belong in the interval $(0, 1)$, i.e., divide the entries by the largest possible value; in this case 255. \n",
    "\n",
    "## One-Hot Encoding \n",
    "\n",
    "<img src=\"onehot.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following code scales our training and testing data, reshapes our images and stores them in new variable names, and one-hot encodes the labels. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data for training. \n",
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "\n",
    "# Flatten the training images into coloumn vectors. \n",
    "flat_train_X = []\n",
    "# One hot encode the training labels\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(train_X, train_y):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "   \n",
    "\n",
    "# Do the same for the testing data \n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(test_X, test_y):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the Network Architecture \n",
    "For our purposes, we will build a multilayered **fully connected**, or **dense**, neural network with $L$ layers, $784$ input notes, $L-2$ hidden layers of arbitrary size, and $10$ output nodes. \n",
    "\n",
    "<img src=\"multilayerPerceptron.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "For our activation function, we will use the sigmoid function:\n",
    "\n",
    "* Sigmoid Function\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "For our cost function, we will use the Mean Sqaure Error cost:\n",
    "$$\n",
    "C(W, b) = \\frac{1}{2}\\sum_{k=1}^{10}(\\hat{y}^{(i)}_k - y^{(i)}_k)^2.\n",
    "$$\n",
    "\n",
    "Our goal will be to write a custom Python class implementing our desired structure. However, before doing so, we first sequentually write functions to better understand the process of programming the following:\n",
    "\n",
    "* Initializing the weights and biases of each layer\n",
    "* The feedforward phase\n",
    "* Calculation of the cost function\n",
    "* Calculation of the gradient\n",
    "* Iterating stochastic gradient descent\n",
    "\n",
    "First we will define our sigmoid activation function, its derivative, and the mean squared error for a single instance of training data. Do this by running the following code. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return .5*sum((a[i] - y[i])**2 for i in range(10))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we will write a custom function to initialize the weight matrices and bias column vectors for a dense neural network. Do this by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "    # The following Python lists will contain numpy matrices\n",
    "    # connected the layers in the neural network \n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        # The scalling factor is something I found in a research paper :)\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "    \n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feedforward Phase\n",
    "\n",
    "For $\\ell = 1, \\dots, L$, each layer $\\ell$ in our network will have two phases, the preactivation phase $$\\mathbf{z}^{\\ell} = W^{\\ell}\\mathbf{a}^{\\ell-1} + \\mathbf{b}^{\\ell},$$ and postactivation phase $$\\mathbf{a}^{\\ell} = \\sigma(\\mathbf{z}^{\\ell}).$$ The preactivation phase consists of a weighted linear combination of postactivation values in the previous layer. The postactivation values consists of passing the preactivation value through an activation function elementwise. Note $\\mathbf{a}^0 = \\mathbf{x}^{(i)}$, where $\\mathbf{x}^{(i)}$ is the current input data into our network. \n",
    "\n",
    "We can test our activation functions and matrix dimensions by running the following code which manually implements the feedforward process on a neural network with the given dimensions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()\n",
    "\n",
    "xi = flat_train_X[0]\n",
    "yi = onehot_train_y[0]\n",
    "a0 = xi\n",
    "\n",
    "print(f\"np.shape(a0) = {np.shape(a0)} \\n\")\n",
    "\n",
    "z1 = W[1] @ a0 + B[1]\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "print(f\"np.shape(W[1]) = {np.shape(W[1])}\")\n",
    "print(f\"np.shape(z1) = {np.shape(z1)}\")\n",
    "print(f\"np.shape(a1) = {np.shape(a1)} \\n\")\n",
    "\n",
    "z2 = W[2] @ a1 + B[2]\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(f\"np.shape(W[2]) = {np.shape(W[2])}\")\n",
    "print(f\"np.shape(z2) = {np.shape(z2)}\")\n",
    "print(f\"np.shape(a2) = {np.shape(a2)} \\n\")\n",
    "\n",
    "z3 = W[3] @ a2 + B[3]\n",
    "a3 = sigmoid(z3)\n",
    "y_hat = a3\n",
    "print(f\"np.shape(W[3]) = {np.shape(W[3])}\")\n",
    "print(f\"np.shape(z3) = {np.shape(z3)}\")\n",
    "print(f\"np.shape(a3) = {np.shape(a3)} \\n\")\n",
    "\n",
    "\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Obviously we could wrap all these lines of code into a ```for```-loop and place it in a callable function. The following code does precisely this. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, B, xi, predict_vector = False):\n",
    "    # pre-activation values\n",
    "    Z = [[0.0]]\n",
    "    # post activation values \n",
    "    A = [xi]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "        \n",
    "    if predict_vector == False:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1] # return the last post-activation , i.e. prediction \n",
    "\n",
    "def predict(W, B, xi):\n",
    "    _, A = forward_pass(W, B, xi)\n",
    "    return np.argmax(A[-1])\n",
    "\n",
    "y_hat = forward_pass(W, B, flat_train_X[0], predict_vector=True)\n",
    "print(f\"Prediction: np.argmax(y_hat) = {np.argmax(y_hat)}\")\n",
    "print(f\"Target Label: np.argmax(yi) = {np.argmax(yi)}\")\n",
    "print(f\"mse(y_hat, yi) = {mse(y_hat, yi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let us next write a custom function for making a prediction on a random data point, as well as write a mean squared error function that computes the error over an entire set of features and labels.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_experiment(W, B, data_features, data_labels):\n",
    "    i = np.random.randint(len(data_features))\n",
    "    print(f\"Actual label: {np.argmax(data_labels[i])}\")\n",
    "    print(f\"Predicted label: {predict(W, B, data_features[i])}\")\n",
    "    \n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    m = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        m+=1\n",
    "    return cost/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE(W, B, flat_train_X, onehot_train_y) = {MSE(W, B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "random_experiment(W, B, flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backpropogation Phase with Stochastic Gradient Descent \n",
    "We are now ready to define a custom Python ```DenseNetwork``` class which initializes the weights and bias for the network, and implements stochastic gradient descent shown below:\n",
    "\n",
    "1. For each $i = 1, \\dots, N$.\n",
    "2. Feedforward $\\mathbf{x}^{(i)}$ into the network. \n",
    "3. Compute $\\delta^{L} = \\nabla_aC\\otimes \\sigma'(\\mathbf{z}^{L})$.\n",
    "4. For $\\ell = L-1, \\dots, 1$, compute $\\delta^{\\ell} = \\big ( (\\mathbf{w}^{\\ell + 1})^{T} \\delta^{\\ell + 1} \\Big )\\otimes \\sigma'(\\mathbf{z}^{\\ell})$.\n",
    "5. For $\\ell = L, L-1, \\dots, 1$, \n",
    "\n",
    "$$\n",
    "w^{\\ell} \\leftarrow w^{\\ell} - \\alpha \\delta^{\\ell}(\\mathbf{a}^{\\ell-1})^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{\\ell} \\leftarrow b^{\\ell} - \\alpha \\delta^{\\ell}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers = self.layers)\n",
    "\n",
    "    def train(self, X_train, y_train, alpha = 0.046, epochs = 4):\n",
    "        # Print the initial mean squared error\n",
    "        self.errors_ = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors_[0]}\")\n",
    "\n",
    "        # Find your sample size\n",
    "        sample_size = len(X_train)\n",
    "\n",
    "        # Find the number of non-input layers.\n",
    "        L = len(self.layers) - 1\n",
    "\n",
    "        # For each epoch perform stochastic gradient descent. \n",
    "        for k in range(epochs):\n",
    "            # Loop over each (xi, yi) training pair of data.\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                # Compute the output error \n",
    "                output_error = (A[L] - yi)*d_sigmoid(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function \n",
    "                # is non-inclusive. \n",
    "                for i in range(L-1, 0, -1):\n",
    "                    # Compute the node errors at each hidden layer\n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*d_sigmoid(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient \n",
    "                # descent. \n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the user the cost over all training examples\n",
    "            self.errors_.append(MSE(self.W, self.B, X_train, y_train))   \n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors_[-1]}\")\n",
    "    \n",
    "\n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a network with 784 input nodes, two hidden layers with 60 nodes each \n",
    "# and a output layer with 10 nodes. \n",
    "net = DenseNetwork(layers = [784, 120, 145, 120, 10])\n",
    "\n",
    "# Check the mean squared error before training \n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "\n",
    "# Make a random prediction before training\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your network with stochastic gradient descent!\n",
    "net.train(flat_train_X, onehot_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean squared error over the training process \n",
    "plt.figure(figsize = (10, 8))\n",
    "epochs = range(len(net.errors_))\n",
    "plt.plot(epochs, net.errors_, marker = \"o\")\n",
    "plt.xticks(epochs)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Network MSE During Training\", fontsize = 16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted digit is: {prediction}\")\n",
    "print(f\"actual digit is: {np.argmax(onehot_test_y[i])}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification Error\n",
    "Let us now calculate the classification percentage on the testing data for our trained dense neural network. Recall that this is simply the number of correct labels divided by the total number of data points:\n",
    "\n",
    "This can be done by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the classification accuracy of our trained model on the test data (I bet we did well!)\n",
    "sum([int(net.predict(x) == y) for x, y in zip(flat_test_X, test_y)])/len(onehot_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "60d347b1b00434e3109b3f59c5a731168507f4d33f1c02fc1f75d9db6931c10a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
